{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXhuDR4Tx60S"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPH6Yu6fx3dP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "outputId": "a41653ac-428c-4699-c698-5801321e766d"
      },
      "source": [
        "!pip install hazm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.10.0-py3-none-any.whl (892 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flashtext<3.0,>=2.7 (from hazm)\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.2)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy==1.24.3 (from hazm)\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=b9a95d552a53a52b934743c9b450d57b91ebd944d70c6538e451517d3f0ef2ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: python-crfsuite, flashtext, pybind11, numpy, fasttext-wheel, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.10.0 numpy-1.24.3 pybind11-2.11.1 python-crfsuite-0.9.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVswsNR6yDK6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import gc\n",
        "import string\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "import hazm\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "from termcolor import colored\n",
        "from itertools import chain\n",
        "#from transformers import BertTokenizer, BertModel\n",
        "who_am_i = 'Mitra'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHQJ8kr1yIRQ"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCB4yVh4yG6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb8a7a8-fe9e-4a7c-b41a-bfd9e66a5c06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'drive/MyDrive/Prose2Poem'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kPwhFZ103-4",
        "outputId": "31e18e81-66ee-4065-9c9c-79ab4d8ced8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1csJsmFwDEwLuf4OVVoqnOnSHZeZvxa2a/Prose2Poem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezEBwcByJkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e5ecbec-14d6-4bb7-f13d-0b54b21b6a61"
      },
      "source": [
        "all_data = pd.read_csv('./Data/Parallel Dataset/ProsPoemParallelDataset_augmented.csv')\n",
        "\n",
        "print('length of augmented cleaned data: ',\n",
        "      colored(len(all_data), 'blue'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of augmented cleaned data:  28820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgDZab2T9vSl"
      },
      "source": [
        "#Me\n",
        "train_size = int(0.8 * len(all_data))\n",
        "train_indices = [i for i in range(train_size)]\n",
        "val_indices = [i for i in range(train_size, len(all_data))]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzschhEx10iz",
        "outputId": "f653c889-c628-4988-b391-e257d9984a77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23056"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFA2ujdT14Fe",
        "outputId": "b4d9254e-22eb-4273-bd8f-4da8dc7d8a25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5764"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CuioUsQ9vPG"
      },
      "source": [
        "# val_indices = pd.read_pickle('.../validation_indices.pickle')\n",
        "# train_indices = pd.read_pickle('.../train_indices.pickle')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35B_eDiUHtBZ"
      },
      "source": [
        "def clean(t):\n",
        "    t = re.sub('^ ', '', t)\n",
        "    t = re.sub(' $', '', t)\n",
        "    t = re.sub(r' */ *', ' / ', t)\n",
        "    t = t.replace('\\\\', '')\n",
        "    t = re.sub(r' \\. *\\.', '\\.', t)\n",
        "    t = re.sub(' +\\s', ' ', t)\n",
        "\n",
        "    t = re.sub(' \\.$', '\\.', t)\n",
        "    t = re.sub('^ *\\. *', '', t)\n",
        "\n",
        "    t = re.sub('[۱۲۳۴۵۶۷۸۹۰]', '', t)\n",
        "\n",
        "    return t\n",
        "\n",
        "all_data.loc[:, 'poetry'] = all_data.loc[:, 'poetry'].apply(lambda x: clean(x))\n",
        "all_data.loc[:, 'text'] = all_data.loc[:, 'text'].apply(lambda x: clean(x))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17w3_2PmzgPE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "edb81629-ae88-4b8c-c43b-c233ab74712c"
      },
      "source": [
        "all_data.head(2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                   poetry  \\\n",
              "0  دوست دارم که بپوشی رخ همچون قمرت / تا چو خورشید نبینند به هر بام و درت   \n",
              "1  جرم بیگانه نباشد که تو خود صورت خویش / گر در آیینه ببینی برود دل ز برت   \n",
              "\n",
              "                                                                                               text  \n",
              "0        دوست دارم که چهرۀ چون ماه تابانت را بپوشانی تا مردم تو را مثل خورشید در کوی و برزن نبینند.  \n",
              "1  بیگانه در عشق ورزیدن به تو گناهی ندارد زیرا اگر تو هم در آینه به چهرۀ خود بنگری دل از کف می‌دهی.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f2a67964-a93a-45c1-b16b-a51ef433c7b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poetry</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>دوست دارم که بپوشی رخ همچون قمرت / تا چو خورشید نبینند به هر بام و درت</td>\n",
              "      <td>دوست دارم که چهرۀ چون ماه تابانت را بپوشانی تا مردم تو را مثل خورشید در کوی و برزن نبینند.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>جرم بیگانه نباشد که تو خود صورت خویش / گر در آیینه ببینی برود دل ز برت</td>\n",
              "      <td>بیگانه در عشق ورزیدن به تو گناهی ندارد زیرا اگر تو هم در آینه به چهرۀ خود بنگری دل از کف می‌دهی.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2a67964-a93a-45c1-b16b-a51ef433c7b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f2a67964-a93a-45c1-b16b-a51ef433c7b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f2a67964-a93a-45c1-b16b-a51ef433c7b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f1fd2216-ef09-4aa0-9934-569658427050\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1fd2216-ef09-4aa0-9934-569658427050')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f1fd2216-ef09-4aa0-9934-569658427050 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTCoVKX30GgR"
      },
      "source": [
        "# PreProcessing + Creating Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogKI5G_azk2B"
      },
      "source": [
        "normalizer = hazm.Normalizer(persian_numbers=False)\n",
        "\n",
        "def process_sents(text):\n",
        "\n",
        "    # separate dot or / from text with\n",
        "    # one white space\n",
        "    text = normalizer.normalize(text)\n",
        "\n",
        "    text = re.sub(r'([\\/\\.])', r' \\1', text)\n",
        "\n",
        "    # substitute / with sep between mesras\n",
        "    text = re.sub(r' *\\/ *', ' <sep> ', text)\n",
        "\n",
        "    # substitute any white space with one space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # add start and end tokens\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMtvwQSs0JKG"
      },
      "source": [
        "def tokenize(lang):\n",
        "    # use keras defualt tokenizer\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=''\n",
        "    )\n",
        "    # fit on the vocabulary used in text\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # convert to ids\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                           padding = 'post')\n",
        "\n",
        "    # add sep to the tokenizer\n",
        "    #idx_sep = len(lang_tokenizer.index_word.keys())+1#[-1]\n",
        "\n",
        "    #lang_tokenizer.word_index['<sep>'] = idx_sep\n",
        "    #lang_tokenizer.index_word[idx_sep] = '<sep>'\n",
        "\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh_PWvqN0Ymn"
      },
      "source": [
        "def create_load_dataset(df):\n",
        "\n",
        "    input_lang = df.loc[:, 'text'].values.tolist()\n",
        "    target_lang = df.loc[:, 'poetry'].values.tolist()\n",
        "\n",
        "    # preprocess each sentence\n",
        "    input_lang = [process_sents(text) for text in input_lang]\n",
        "    target_lang = [process_sents(text) for text in target_lang]\n",
        "\n",
        "    # create a tensor and tokenizer for each language\n",
        "    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUXdY3_o0J0I"
      },
      "source": [
        "input_tensor, target_tensor,\\\n",
        "input_lang_tokenizer, target_lang_tokenizer = create_load_dataset(all_data)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUercjr_2uKJ"
      },
      "source": [],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF8FjLTV0JyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efe5f07-9172-478b-9293-2c69adf5ba09"
      },
      "source": [
        "max_len_input = input_tensor.shape[1]\n",
        "max_len_target = target_tensor.shape[1]\n",
        "\n",
        "print('longest sequence and the length of texts: ',\n",
        "      colored(max_len_input, 'blue'))\n",
        "print('longest sequence and the length of poetries: ',\n",
        "      colored(max_len_target, 'blue'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "longest sequence and the length of texts:  70\n",
            "longest sequence and the length of poetries:  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl7qZtXP4irR"
      },
      "source": [],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlXGwLTv1cl5"
      },
      "source": [
        "# Vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPftSMTr1ZqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f236687-073b-4aae-ef42-f8a685c4067f"
      },
      "source": [
        "# lenght of constructed vocabularies:\n",
        "# 1 for padding\n",
        "vocab_len_i = len(input_lang_tokenizer.index_word) + 1\n",
        "print(\"Plain text vocab has\", colored(f\"{vocab_len_i:,}\", 'green'), \"unique words.\")\n",
        "\n",
        "vocab_len_t = len(target_lang_tokenizer.index_word) + 1\n",
        "print(f\"Poetry vocab has\", colored(f\"{vocab_len_t:,}\", 'green'), \"unique words.\")\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plain text vocab has 18,107 unique words.\n",
            "Poetry vocab has 11,807 unique words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhj56O291e1x"
      },
      "source": [
        "def convert(text, poetry):\n",
        "\n",
        "\n",
        "    print(colored('Text:', 'green'))\n",
        "    for i in text:\n",
        "        if i!=0:\n",
        "            print(\"%d -----> %s\"%(i, input_lang_tokenizer.index_word[i]))\n",
        "\n",
        "    print(colored('\\nPoetry:', 'green'))\n",
        "    for i in poetry:\n",
        "        if i!=0:\n",
        "            print(\"%d -----> %s\"%(i, target_lang_tokenizer.index_word[i]))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92tVL-8g1vUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53a8cf7-35e1-4976-a90d-679a09c0848c"
      },
      "source": [
        "print(colored('Text: ', 'blue'), all_data.loc[5, 'text'])\n",
        "print(colored('Poetry: ', 'blue'), all_data.loc[5, 'poetry'])\n",
        "convert(input_tensor[5], target_tensor[5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  به دفعات و مکرر گفته‌ام که چهرۀ زیبای خود را پیش هر کسی آشکار مساز تا هر انسان بی بصیرتی به آن چهره ننگرد.\n",
            "Poetry:  بارها گفته‌ام این روی به هر کس منمای / تا تأمل نکند دیدۀ هر بی بصرت\n",
            "Text:\n",
            "3 -----> <start>\n",
            "5 -----> به\n",
            "10902 -----> دفعات\n",
            "1 -----> و\n",
            "8419 -----> مکرر\n",
            "13580 -----> گفته‌ام\n",
            "6 -----> که\n",
            "1028 -----> چهرۀ\n",
            "1003 -----> زیبای\n",
            "15 -----> خود\n",
            "8 -----> را\n",
            "105 -----> پیش\n",
            "21 -----> هر\n",
            "29 -----> کسی\n",
            "510 -----> آشکار\n",
            "13581 -----> مساز\n",
            "26 -----> تا\n",
            "21 -----> هر\n",
            "73 -----> انسان\n",
            "86 -----> بی\n",
            "5626 -----> بصیرتی\n",
            "5 -----> به\n",
            "17 -----> آن\n",
            "604 -----> چهره\n",
            "5268 -----> ننگرد\n",
            "2 -----> .\n",
            "4 -----> <end>\n",
            "\n",
            "Poetry:\n",
            "1 -----> <start>\n",
            "4532 -----> بارها\n",
            "9452 -----> گفته‌ام\n",
            "14 -----> این\n",
            "44 -----> روی\n",
            "10 -----> به\n",
            "20 -----> هر\n",
            "103 -----> کس\n",
            "9453 -----> منمای\n",
            "3 -----> <sep>\n",
            "19 -----> تا\n",
            "3947 -----> تأمل\n",
            "538 -----> نکند\n",
            "2001 -----> دیدۀ\n",
            "20 -----> هر\n",
            "41 -----> بی\n",
            "6839 -----> بصرت\n",
            "2 -----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBJAjAwD1IJa"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8jm6TxX5R6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bffe859-0db5-4761-914b-3d9cd9741205"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val =\\\n",
        "input_tensor[train_indices], input_tensor[val_indices],  target_tensor[train_indices], target_tensor[val_indices]\n",
        "\n",
        "print('Length of train and val:',\n",
        "      colored(f\"{len(input_tensor_train), len(input_tensor_val)}\", 'blue'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train and val: (23056, 5764)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u8Iv0As0Jwa"
      },
      "source": [
        "# defining the main parameters of the model\n",
        "# and the inputs\n",
        "\n",
        "len_data = len(input_tensor_train)\n",
        "batch_s = 256\n",
        "steps_per_epoch = len_data // batch_s\n",
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVyXi7KO0JuQ"
      },
      "source": [
        "# create the dataset and shuffle all\n",
        "len_data_train = len(input_tensor_train)\n",
        "len_data_test = len(target_tensor_val)\n",
        "\n",
        "# creat the datasets and put them in batches\n",
        "\n",
        "train_batches = tf.data.Dataset.from_tensor_slices((\n",
        "    np.array(input_tensor_train.tolist(), dtype='int32'),\n",
        "     np.array(target_tensor_train.tolist(), dtype='int32')\n",
        ")).shuffle(len_data_train).batch(batch_s, drop_remainder=True)\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQvHp6i1FPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209a4731-5bad-4d20-b885-5f930d52e8b2"
      },
      "source": [
        "input_batch_sample, target_batch_sample = next(iter(train_batches))\n",
        "\n",
        "print('A sample of text(input) batch: \\n',\n",
        "      colored(input_batch_sample, 'blue'))\n",
        "\n",
        "print('\\nA sample of poetry(target) batch: \\n',\n",
        "      colored(target_batch_sample, 'blue'))\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sample of text(input) batch: \n",
            " tf.Tensor(\n",
            "[[   3   24 8379 ...    0    0    0]\n",
            " [   3  128  487 ...    0    0    0]\n",
            " [   3   12   27 ...    0    0    0]\n",
            " ...\n",
            " [   3  110    9 ...    0    0    0]\n",
            " [   3   16   18 ...    0    0    0]\n",
            " [   3 3468  581 ...    0    0    0]], shape=(256, 70), dtype=int32)\n",
            "\n",
            "A sample of poetry(target) batch: \n",
            " tf.Tensor(\n",
            "[[   1  282  217 ...    0    0    0]\n",
            " [   1  139 1716 ...    0    0    0]\n",
            " [   1  220   25 ...    0    0    0]\n",
            " ...\n",
            " [   1   16 2841 ...    0    0    0]\n",
            " [   1   80   72 ...    0    0    0]\n",
            " [   1  226  872 ...    0    0    0]], shape=(256, 25), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdaWqqGe1FMo"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 enc_units, batch_s, ):\n",
        "\n",
        "        # change the primary model from keras\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # define the parameters\n",
        "        self.batch_s = batch_s\n",
        "        self.enc_units = enc_units\n",
        "        self.embeddings = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # first part of the model\n",
        "        # calling the embeddings and giving them\n",
        "        # to the gru\n",
        "        x = self.embeddings(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # the initial state of the hidden states\n",
        "        # start with zeros\n",
        "        return tf.zeros((self.batch_s, self.enc_units))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z11DHGRm1FKi"
      },
      "source": [
        "encoder = Encoder(vocab_len_i, embedding_dim, units, batch_s)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8thv9s91FFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315d303b-4f41-46b2-848c-6915f23a3cef"
      },
      "source": [
        "# initialize the hs\n",
        "sample_hidden_states = encoder.initialize_hidden_state()\n",
        "# get the output of the encoder\n",
        "sample_encoder_output, sample_hidden_states_encoder = encoder(input_batch_sample, sample_hidden_states)\n",
        "\n",
        "print('Encoder hidden states shapes:',\n",
        "      colored(sample_hidden_states_encoder.shape, 'blue'))\n",
        "print('Encoder output shape:',\n",
        "      colored(sample_encoder_output.shape, 'blue'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder hidden states shapes: (256, 1024)\n",
            "Encoder output shape: (256, 70, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSuX60f41FDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c789d135-7ee0-4837-b611-1ecf983a32a9"
      },
      "source": [
        "print('Hidden states after being processed in gru:\\n',\n",
        "      colored(sample_hidden_states_encoder, 'blue'))\n",
        "\n",
        "print('\\nEncoder output sample:\\n', colored(sample_encoder_output, 'blue'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden states after being processed in gru:\n",
            " tf.Tensor(\n",
            "[[-0.01696949  0.02688834 -0.02156566 ... -0.00509517  0.00107555\n",
            "  -0.02101996]\n",
            " [-0.01696949  0.02688834 -0.02156566 ... -0.00509517  0.00107555\n",
            "  -0.02101996]\n",
            " [-0.01696949  0.02688834 -0.02156566 ... -0.00509517  0.00107555\n",
            "  -0.02101996]\n",
            " ...\n",
            " [-0.01696949  0.02688834 -0.02156566 ... -0.00509518  0.00107555\n",
            "  -0.02101996]\n",
            " [-0.01696949  0.02688834 -0.02156566 ... -0.00509517  0.00107555\n",
            "  -0.02101996]\n",
            " [-0.01696949  0.02688834 -0.02156566 ... -0.00509517  0.00107555\n",
            "  -0.02101996]], shape=(256, 1024), dtype=float32)\n",
            "\n",
            "Encoder output sample:\n",
            " tf.Tensor(\n",
            "[[[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [-6.05875952e-03  3.70479957e-03  7.06838211e-03 ... -6.75688498e-03\n",
            "    6.06072461e-03 -2.47711758e-03]\n",
            "  [-3.65584344e-03 -3.07724666e-04  4.79982747e-03 ...  4.74262750e-03\n",
            "    3.28678824e-03 -3.03384103e-03]\n",
            "  ...\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517500e-03\n",
            "    1.07554707e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517454e-03\n",
            "    1.07554672e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517454e-03\n",
            "    1.07554684e-03 -2.10199598e-02]]\n",
            "\n",
            " [[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [ 3.94996069e-03  1.52085081e-03 -5.93836233e-03 ...  3.49375699e-03\n",
            "    4.04548598e-03  9.97385103e-03]\n",
            "  [ 7.98419490e-03  2.16553826e-03 -1.34255621e-03 ... -9.88433231e-03\n",
            "    9.61994193e-03 -1.74498872e-03]\n",
            "  ...\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517360e-03\n",
            "    1.07554905e-03 -2.10199580e-02]\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517407e-03\n",
            "    1.07554847e-03 -2.10199580e-02]\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517360e-03\n",
            "    1.07554730e-03 -2.10199580e-02]]\n",
            "\n",
            " [[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [-3.96254892e-03  1.35944423e-03 -4.72536683e-03 ... -5.88865299e-03\n",
            "   -7.89713394e-03  8.31252150e-03]\n",
            "  [ 2.65851850e-03  4.84547950e-03 -6.20546192e-03 ... -9.81822517e-03\n",
            "   -5.41210501e-03  7.66614778e-03]\n",
            "  ...\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517407e-03\n",
            "    1.07554649e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517454e-03\n",
            "    1.07554649e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517454e-03\n",
            "    1.07554672e-03 -2.10199598e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [ 7.03631714e-03  1.00727938e-03 -4.21478413e-03 ... -6.13731286e-03\n",
            "    1.39604695e-02 -1.22596435e-02]\n",
            "  [-6.02484774e-03  4.67850547e-03  4.59794234e-03 ... -1.65732915e-03\n",
            "    7.91816413e-03 -1.15882242e-02]\n",
            "  ...\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517500e-03\n",
            "    1.07554707e-03 -2.10199598e-02]\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517500e-03\n",
            "    1.07554707e-03 -2.10199598e-02]\n",
            "  [-1.69694945e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517500e-03\n",
            "    1.07554649e-03 -2.10199598e-02]]\n",
            "\n",
            " [[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [-1.09708461e-03 -5.19044232e-03  3.48938350e-03 ...  1.82005903e-03\n",
            "   -2.59579485e-03 -2.74725990e-05]\n",
            "  [-6.79036777e-04  1.14501046e-03  6.20604842e-04 ...  5.84933162e-03\n",
            "   -2.25989544e-03  7.80329062e-03]\n",
            "  ...\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517500e-03\n",
            "    1.07554684e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517500e-03\n",
            "    1.07554684e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656571e-02 ... -5.09517454e-03\n",
            "    1.07554637e-03 -2.10199598e-02]]\n",
            "\n",
            " [[-3.74829234e-03  1.00068878e-02 -7.89913058e-04 ... -1.32240565e-03\n",
            "    3.14204372e-03 -2.22523790e-03]\n",
            "  [ 3.35548721e-05  4.30029072e-03  1.54477948e-05 ... -2.87423154e-05\n",
            "    5.41283190e-03  9.40916687e-03]\n",
            "  [-8.73586535e-03  9.72643308e-03  2.67056213e-03 ...  2.21317122e-03\n",
            "   -2.86099431e-03  4.50384663e-03]\n",
            "  ...\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517454e-03\n",
            "    1.07554800e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517454e-03\n",
            "    1.07554730e-03 -2.10199598e-02]\n",
            "  [-1.69694927e-02  2.68883388e-02 -2.15656590e-02 ... -5.09517454e-03\n",
            "    1.07554730e-03 -2.10199598e-02]]], shape=(256, 70, 1024), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTafjWk71E85"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    # create the decoder side\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 decoder_units, batch_s, ):\n",
        "\n",
        "        # take and change the keras.model\n",
        "        super(Decoder, self).__init__()\n",
        "        # parameters\n",
        "        self.batch_s = batch_s\n",
        "        self.decoder_units = decoder_units\n",
        "        self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.gru = tf.keras.layers.GRU(self.decoder_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        # fully connected\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size,\n",
        "                                        activation='softmax')\n",
        "\n",
        "        self.attention = BahdanauAttention(self.decoder_units)\n",
        "\n",
        "    def call(self, x, hidden, encoder_output):\n",
        "        # construct the decoder\n",
        "\n",
        "        # x shape = (batch_s, 1)\n",
        "        # one id for each word from the target\n",
        "\n",
        "        # encoder output = (batch_s, max_len, hidden_states_s)\n",
        "        context_vector = self.attention(query=hidden,\n",
        "                                                           value=encoder_output)\n",
        "\n",
        "        # we expand the ids into embedding vectors\n",
        "        # x = (batch_s, 1, embedding_dim)\n",
        "        x = self.embeddings(x)\n",
        "\n",
        "        # concatenating hidden states and the context\n",
        "        # vector\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x],\n",
        "                      axis=-1)\n",
        "\n",
        "        # give both attention and embeddings to gru\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output = (batch_size, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # batch_s, vocab_size\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70UphWDE1E6h"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, value):\n",
        "        # query = batch_s, hidden_states - from decoder\n",
        "        # query_with_time = batch_s, 1, hidden_states\n",
        "        # values = batch_s, max_len_input, hidden_states\n",
        "\n",
        "        # adding one dimention to take time into account\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "\n",
        "        # combining values and queries\n",
        "        # attention scores = batch_s, max_len, 1\n",
        "        attention_score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(value)\n",
        "        ))\n",
        "\n",
        "        # getting a softmax to choose the weights for\n",
        "        # each position in input\n",
        "        # batch_s, max_len_input, 1\n",
        "        attention_weights = tf.nn.softmax(attention_score, axis=1)\n",
        "\n",
        "        # after multiplication and summing:\n",
        "        # context_vector = batch_s, hidden_s\n",
        "        context_vector = attention_weights * value\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNM2yJQW1E4Q"
      },
      "source": [
        "decoder = Decoder(vocab_len_t, embedding_dim, units, batch_s)\n",
        "\n",
        "\n",
        "sample_decoder_output, states = decoder(x = tf.random.uniform((batch_s, 1)),\n",
        "                                        hidden = sample_hidden_states_encoder,\n",
        "                                        encoder_output = sample_encoder_output)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NssDHW756Dwi"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiHTvQRC1E1x"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "\n",
        "    # first mask the ones that are not paddings\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "\n",
        "    # apply the loss on the whole sequence\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # make the mask datatype the same as loss\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "\n",
        "    loss_ = loss_ * mask\n",
        "\n",
        "    # return the mean of all words\n",
        "    return tf.reduce_mean(loss_)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d561K3pc0JsA"
      },
      "source": [
        "\n",
        "\n",
        "# Switching from eager execution to graph execution\n",
        "\n",
        "@tf.function\n",
        "def train_step(input, target, encoder_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        # get to the encoder\n",
        "        encoder_output, encoder_hidden_states = encoder(input, encoder_hidden)\n",
        "\n",
        "        # start the decoder\n",
        "        decoder_hidden_states = encoder_hidden_states\n",
        "\n",
        "        # give <s> to all as the first word\n",
        "        decoder_input = \\\n",
        "        tf.expand_dims([target_lang_tokenizer.word_index['<start>']] * batch_s, 1)\n",
        "\n",
        "    # Teacher Forcing\n",
        "    # start from the first word and continue\n",
        "    # till the end of the sequence\n",
        "\n",
        "        for t in range(1, target.shape[1]):\n",
        "\n",
        "            # give\n",
        "            # 1. decoder input that starts with <start>\n",
        "            # 2. decoder hidden states\n",
        "            # 3. encoder output\n",
        "            # to the decoder\n",
        "\n",
        "            Seq2Seq_logits, decoder_hidden_states = decoder(\n",
        "                decoder_input, decoder_hidden_states, encoder_output\n",
        "            )\n",
        "\n",
        "            # ------------------------------------\n",
        "            # calculate the loss at time step t\n",
        "            loss += loss_function(target[:, t], Seq2Seq_logits)\n",
        "\n",
        "            # change the decoder input to the target token of\n",
        "            # this time step for\n",
        "            # Teacher Forcing\n",
        "            decoder_input = tf.expand_dims(target[:, t], axis=1)\n",
        "\n",
        "    # get the mean loss\n",
        "    batch_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    # get the variables that have been changed\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # calculate the gradients based on the loss\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1-V2mn0Jp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276ee5ce-83bb-40cb-db01-940e6fba8a32"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIzew4sr6biA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5915fb44-62f6-4182-a9d9-5cdfcb075c0f"
      },
      "source": [
        "# alpha = 0.6\n",
        "epochs =\n",
        "start_id = '<start>'\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print(\"epoch \", colored(epoch, 'blue'))\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_hidden_state = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (input, target)) in enumerate(train_batches.take(steps_per_epoch)):\n",
        "\n",
        "        batch_loss = train_step(input, target, encoder_hidden_state)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        if batch%50==0:\n",
        "            print('batch ', colored(batch, 'green'),\n",
        "                  f' Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "    # checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "    print(f'Time taken: {time.time() - start:.2f} seconds')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  0\n",
            "batch  0  Loss 4.0598\n",
            "batch  50  Loss 3.9139\n",
            "Time taken: 201.91 seconds\n",
            "epoch  1\n",
            "batch  0  Loss 3.7582\n",
            "batch  50  Loss 3.7787\n",
            "Time taken: 129.42 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDNK0ntHR2BC"
      },
      "source": [
        "# Normal Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8DsisU06bf4"
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "\n",
        "    # attention_plot = np.zeros((max_len_target,\n",
        "      #                         max_len_input))\n",
        "\n",
        "    # preprocessing every sentence before giving\n",
        "    # them to the model\n",
        "    sentence = process_sents(sentence)\n",
        "    # converting str to ids and padding and creating a tensor\n",
        "    # from all\n",
        "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_len_input,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    # start constructing the output string\n",
        "    output = ''\n",
        "\n",
        "    hidden_state = [tf.zeros((1, units))]\n",
        "    encoder_output, encoder_hidden_state = encoder(inputs, hidden_state)\n",
        "\n",
        "    decoder_hidden_state = encoder_hidden_state\n",
        "\n",
        "    decoder_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "\n",
        "    for t in range(max_len_target):\n",
        "        predictions, decoder_hidden_state = decoder(\n",
        "            decoder_input, decoder_hidden_state,\n",
        "            encoder_output\n",
        "        )\n",
        "\n",
        "\n",
        "        # sotring for plot\n",
        "        # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        # attention_plot[t] = attention_weights.numpy()\n",
        "        # predict the most probable token\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # add this token to the previous ones\n",
        "        output = output + target_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        # is it over?\n",
        "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return output, sentence\n",
        "\n",
        "        # give the prediction to continue predicting\n",
        "        # next tokens\n",
        "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "\n",
        "    return output, sentence"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQpUMwo-6bd5"
      },
      "source": [
        "def transform(text):\n",
        "    output, text = evaluate(sentence=text)\n",
        "\n",
        "    print('Text: ', text)\n",
        "    print('Generate poetry:', output)\n",
        "\n",
        "\n",
        "    # attention_plot = attention_plot[:len(output.split(' ')),\n",
        "     #                             :len(text.split(' '))]\n",
        "    # plot_attention(attention_plot, text.split(' '), output.split(' '))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VAB3c-GU3xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991ce0ea-957d-4f73-d944-74b36fa86194"
      },
      "source": [
        "transform('با این توصیف عشاق بی عقل و بدون هدف خاص زندگی می کنند و دارای هیچ هدف و مغزی نیستند تا اینکه به جهنم می رسند و به هیچ جایگاه دنیوی و واقعی دست پیدا نمی کنند')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  <start> با این توصیف عشاق بی‌عقل و بدون هدف خاص زندگی می‌کنند و دارای هیچ هدف و مغزی نیستند تا اینکه به جهنم می‌رسند و به هیچ جایگاه دنیوی و واقعی دست پیدا نمی‌کنند <end>\n",
            "Generate poetry: تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91GOZVAlBUk1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2MZMp_i_6u1"
      },
      "source": [
        "# evaluate a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO7UZz-sKXDD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JnCHQ2UKU6D"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UsFlIxyKT6b"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvzhFZzgU3sI"
      },
      "source": [
        "def evaluate_dataset(df):\n",
        "\n",
        "    generated_p = []\n",
        "\n",
        "    df = df.reset_index(drop=True)\n",
        "    for r in range(len(df)):\n",
        "\n",
        "        try:\n",
        "            # attention_plot = np.zeros((max_len_target,\n",
        "            #                         max_len_input))\n",
        "\n",
        "            # preprocessing every sentence before giving\n",
        "            # them to the model\n",
        "            sentence = process_sents(df.loc[r, 'text'])\n",
        "            # converting str to ids and padding and creating a tensor\n",
        "            # from all\n",
        "            inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "            inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                                maxlen=max_len_input,\n",
        "                                                                padding='post')\n",
        "            inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "            # start constructing the output string\n",
        "            output = ''\n",
        "\n",
        "            hidden_state = [tf.zeros((1, units))]\n",
        "            encoder_output, encoder_hidden_state = encoder(inputs, hidden_state)\n",
        "\n",
        "            decoder_hidden_state = encoder_hidden_state\n",
        "\n",
        "            decoder_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "\n",
        "            for t in range(max_len_target):\n",
        "                predictions, decoder_hidden_state = decoder(\n",
        "                    decoder_input, decoder_hidden_state,\n",
        "                    encoder_output\n",
        "                )\n",
        "\n",
        "\n",
        "                # sotring for plot\n",
        "                # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "                # attention_plot[t] = attention_weights.numpy()\n",
        "                # predict the most probable token\n",
        "                predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "                # add this token to the previous ones\n",
        "                output = output + target_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "                # is it over?\n",
        "                if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "                    break\n",
        "\n",
        "                # give the prediction to continue predicting\n",
        "                # next tokens\n",
        "                decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "            generated_p.append(output)\n",
        "\n",
        "        except:\n",
        "            print(r)\n",
        "            print(df.loc[r, 'text'])\n",
        "\n",
        "            generated_p.append(None)\n",
        "\n",
        "    df_output = pd.concat([df, pd.Series(generated_p)],\n",
        "                                axis = 1)\n",
        "\n",
        "    df_output.columns = ['poetry_ground_truth',\n",
        "                        'text',\n",
        "                        'poetry_generated_Seq2Seq_with_Att']\n",
        "\n",
        "\n",
        "    return df_output"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk1FnbgbMDDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae9ec66-18b1-4fcd-bd86-3743fe2d0c86"
      },
      "source": [
        "val_indices"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[23056,\n",
              " 23057,\n",
              " 23058,\n",
              " 23059,\n",
              " 23060,\n",
              " 23061,\n",
              " 23062,\n",
              " 23063,\n",
              " 23064,\n",
              " 23065,\n",
              " 23066,\n",
              " 23067,\n",
              " 23068,\n",
              " 23069,\n",
              " 23070,\n",
              " 23071,\n",
              " 23072,\n",
              " 23073,\n",
              " 23074,\n",
              " 23075,\n",
              " 23076,\n",
              " 23077,\n",
              " 23078,\n",
              " 23079,\n",
              " 23080,\n",
              " 23081,\n",
              " 23082,\n",
              " 23083,\n",
              " 23084,\n",
              " 23085,\n",
              " 23086,\n",
              " 23087,\n",
              " 23088,\n",
              " 23089,\n",
              " 23090,\n",
              " 23091,\n",
              " 23092,\n",
              " 23093,\n",
              " 23094,\n",
              " 23095,\n",
              " 23096,\n",
              " 23097,\n",
              " 23098,\n",
              " 23099,\n",
              " 23100,\n",
              " 23101,\n",
              " 23102,\n",
              " 23103,\n",
              " 23104,\n",
              " 23105,\n",
              " 23106,\n",
              " 23107,\n",
              " 23108,\n",
              " 23109,\n",
              " 23110,\n",
              " 23111,\n",
              " 23112,\n",
              " 23113,\n",
              " 23114,\n",
              " 23115,\n",
              " 23116,\n",
              " 23117,\n",
              " 23118,\n",
              " 23119,\n",
              " 23120,\n",
              " 23121,\n",
              " 23122,\n",
              " 23123,\n",
              " 23124,\n",
              " 23125,\n",
              " 23126,\n",
              " 23127,\n",
              " 23128,\n",
              " 23129,\n",
              " 23130,\n",
              " 23131,\n",
              " 23132,\n",
              " 23133,\n",
              " 23134,\n",
              " 23135,\n",
              " 23136,\n",
              " 23137,\n",
              " 23138,\n",
              " 23139,\n",
              " 23140,\n",
              " 23141,\n",
              " 23142,\n",
              " 23143,\n",
              " 23144,\n",
              " 23145,\n",
              " 23146,\n",
              " 23147,\n",
              " 23148,\n",
              " 23149,\n",
              " 23150,\n",
              " 23151,\n",
              " 23152,\n",
              " 23153,\n",
              " 23154,\n",
              " 23155,\n",
              " 23156,\n",
              " 23157,\n",
              " 23158,\n",
              " 23159,\n",
              " 23160,\n",
              " 23161,\n",
              " 23162,\n",
              " 23163,\n",
              " 23164,\n",
              " 23165,\n",
              " 23166,\n",
              " 23167,\n",
              " 23168,\n",
              " 23169,\n",
              " 23170,\n",
              " 23171,\n",
              " 23172,\n",
              " 23173,\n",
              " 23174,\n",
              " 23175,\n",
              " 23176,\n",
              " 23177,\n",
              " 23178,\n",
              " 23179,\n",
              " 23180,\n",
              " 23181,\n",
              " 23182,\n",
              " 23183,\n",
              " 23184,\n",
              " 23185,\n",
              " 23186,\n",
              " 23187,\n",
              " 23188,\n",
              " 23189,\n",
              " 23190,\n",
              " 23191,\n",
              " 23192,\n",
              " 23193,\n",
              " 23194,\n",
              " 23195,\n",
              " 23196,\n",
              " 23197,\n",
              " 23198,\n",
              " 23199,\n",
              " 23200,\n",
              " 23201,\n",
              " 23202,\n",
              " 23203,\n",
              " 23204,\n",
              " 23205,\n",
              " 23206,\n",
              " 23207,\n",
              " 23208,\n",
              " 23209,\n",
              " 23210,\n",
              " 23211,\n",
              " 23212,\n",
              " 23213,\n",
              " 23214,\n",
              " 23215,\n",
              " 23216,\n",
              " 23217,\n",
              " 23218,\n",
              " 23219,\n",
              " 23220,\n",
              " 23221,\n",
              " 23222,\n",
              " 23223,\n",
              " 23224,\n",
              " 23225,\n",
              " 23226,\n",
              " 23227,\n",
              " 23228,\n",
              " 23229,\n",
              " 23230,\n",
              " 23231,\n",
              " 23232,\n",
              " 23233,\n",
              " 23234,\n",
              " 23235,\n",
              " 23236,\n",
              " 23237,\n",
              " 23238,\n",
              " 23239,\n",
              " 23240,\n",
              " 23241,\n",
              " 23242,\n",
              " 23243,\n",
              " 23244,\n",
              " 23245,\n",
              " 23246,\n",
              " 23247,\n",
              " 23248,\n",
              " 23249,\n",
              " 23250,\n",
              " 23251,\n",
              " 23252,\n",
              " 23253,\n",
              " 23254,\n",
              " 23255,\n",
              " 23256,\n",
              " 23257,\n",
              " 23258,\n",
              " 23259,\n",
              " 23260,\n",
              " 23261,\n",
              " 23262,\n",
              " 23263,\n",
              " 23264,\n",
              " 23265,\n",
              " 23266,\n",
              " 23267,\n",
              " 23268,\n",
              " 23269,\n",
              " 23270,\n",
              " 23271,\n",
              " 23272,\n",
              " 23273,\n",
              " 23274,\n",
              " 23275,\n",
              " 23276,\n",
              " 23277,\n",
              " 23278,\n",
              " 23279,\n",
              " 23280,\n",
              " 23281,\n",
              " 23282,\n",
              " 23283,\n",
              " 23284,\n",
              " 23285,\n",
              " 23286,\n",
              " 23287,\n",
              " 23288,\n",
              " 23289,\n",
              " 23290,\n",
              " 23291,\n",
              " 23292,\n",
              " 23293,\n",
              " 23294,\n",
              " 23295,\n",
              " 23296,\n",
              " 23297,\n",
              " 23298,\n",
              " 23299,\n",
              " 23300,\n",
              " 23301,\n",
              " 23302,\n",
              " 23303,\n",
              " 23304,\n",
              " 23305,\n",
              " 23306,\n",
              " 23307,\n",
              " 23308,\n",
              " 23309,\n",
              " 23310,\n",
              " 23311,\n",
              " 23312,\n",
              " 23313,\n",
              " 23314,\n",
              " 23315,\n",
              " 23316,\n",
              " 23317,\n",
              " 23318,\n",
              " 23319,\n",
              " 23320,\n",
              " 23321,\n",
              " 23322,\n",
              " 23323,\n",
              " 23324,\n",
              " 23325,\n",
              " 23326,\n",
              " 23327,\n",
              " 23328,\n",
              " 23329,\n",
              " 23330,\n",
              " 23331,\n",
              " 23332,\n",
              " 23333,\n",
              " 23334,\n",
              " 23335,\n",
              " 23336,\n",
              " 23337,\n",
              " 23338,\n",
              " 23339,\n",
              " 23340,\n",
              " 23341,\n",
              " 23342,\n",
              " 23343,\n",
              " 23344,\n",
              " 23345,\n",
              " 23346,\n",
              " 23347,\n",
              " 23348,\n",
              " 23349,\n",
              " 23350,\n",
              " 23351,\n",
              " 23352,\n",
              " 23353,\n",
              " 23354,\n",
              " 23355,\n",
              " 23356,\n",
              " 23357,\n",
              " 23358,\n",
              " 23359,\n",
              " 23360,\n",
              " 23361,\n",
              " 23362,\n",
              " 23363,\n",
              " 23364,\n",
              " 23365,\n",
              " 23366,\n",
              " 23367,\n",
              " 23368,\n",
              " 23369,\n",
              " 23370,\n",
              " 23371,\n",
              " 23372,\n",
              " 23373,\n",
              " 23374,\n",
              " 23375,\n",
              " 23376,\n",
              " 23377,\n",
              " 23378,\n",
              " 23379,\n",
              " 23380,\n",
              " 23381,\n",
              " 23382,\n",
              " 23383,\n",
              " 23384,\n",
              " 23385,\n",
              " 23386,\n",
              " 23387,\n",
              " 23388,\n",
              " 23389,\n",
              " 23390,\n",
              " 23391,\n",
              " 23392,\n",
              " 23393,\n",
              " 23394,\n",
              " 23395,\n",
              " 23396,\n",
              " 23397,\n",
              " 23398,\n",
              " 23399,\n",
              " 23400,\n",
              " 23401,\n",
              " 23402,\n",
              " 23403,\n",
              " 23404,\n",
              " 23405,\n",
              " 23406,\n",
              " 23407,\n",
              " 23408,\n",
              " 23409,\n",
              " 23410,\n",
              " 23411,\n",
              " 23412,\n",
              " 23413,\n",
              " 23414,\n",
              " 23415,\n",
              " 23416,\n",
              " 23417,\n",
              " 23418,\n",
              " 23419,\n",
              " 23420,\n",
              " 23421,\n",
              " 23422,\n",
              " 23423,\n",
              " 23424,\n",
              " 23425,\n",
              " 23426,\n",
              " 23427,\n",
              " 23428,\n",
              " 23429,\n",
              " 23430,\n",
              " 23431,\n",
              " 23432,\n",
              " 23433,\n",
              " 23434,\n",
              " 23435,\n",
              " 23436,\n",
              " 23437,\n",
              " 23438,\n",
              " 23439,\n",
              " 23440,\n",
              " 23441,\n",
              " 23442,\n",
              " 23443,\n",
              " 23444,\n",
              " 23445,\n",
              " 23446,\n",
              " 23447,\n",
              " 23448,\n",
              " 23449,\n",
              " 23450,\n",
              " 23451,\n",
              " 23452,\n",
              " 23453,\n",
              " 23454,\n",
              " 23455,\n",
              " 23456,\n",
              " 23457,\n",
              " 23458,\n",
              " 23459,\n",
              " 23460,\n",
              " 23461,\n",
              " 23462,\n",
              " 23463,\n",
              " 23464,\n",
              " 23465,\n",
              " 23466,\n",
              " 23467,\n",
              " 23468,\n",
              " 23469,\n",
              " 23470,\n",
              " 23471,\n",
              " 23472,\n",
              " 23473,\n",
              " 23474,\n",
              " 23475,\n",
              " 23476,\n",
              " 23477,\n",
              " 23478,\n",
              " 23479,\n",
              " 23480,\n",
              " 23481,\n",
              " 23482,\n",
              " 23483,\n",
              " 23484,\n",
              " 23485,\n",
              " 23486,\n",
              " 23487,\n",
              " 23488,\n",
              " 23489,\n",
              " 23490,\n",
              " 23491,\n",
              " 23492,\n",
              " 23493,\n",
              " 23494,\n",
              " 23495,\n",
              " 23496,\n",
              " 23497,\n",
              " 23498,\n",
              " 23499,\n",
              " 23500,\n",
              " 23501,\n",
              " 23502,\n",
              " 23503,\n",
              " 23504,\n",
              " 23505,\n",
              " 23506,\n",
              " 23507,\n",
              " 23508,\n",
              " 23509,\n",
              " 23510,\n",
              " 23511,\n",
              " 23512,\n",
              " 23513,\n",
              " 23514,\n",
              " 23515,\n",
              " 23516,\n",
              " 23517,\n",
              " 23518,\n",
              " 23519,\n",
              " 23520,\n",
              " 23521,\n",
              " 23522,\n",
              " 23523,\n",
              " 23524,\n",
              " 23525,\n",
              " 23526,\n",
              " 23527,\n",
              " 23528,\n",
              " 23529,\n",
              " 23530,\n",
              " 23531,\n",
              " 23532,\n",
              " 23533,\n",
              " 23534,\n",
              " 23535,\n",
              " 23536,\n",
              " 23537,\n",
              " 23538,\n",
              " 23539,\n",
              " 23540,\n",
              " 23541,\n",
              " 23542,\n",
              " 23543,\n",
              " 23544,\n",
              " 23545,\n",
              " 23546,\n",
              " 23547,\n",
              " 23548,\n",
              " 23549,\n",
              " 23550,\n",
              " 23551,\n",
              " 23552,\n",
              " 23553,\n",
              " 23554,\n",
              " 23555,\n",
              " 23556,\n",
              " 23557,\n",
              " 23558,\n",
              " 23559,\n",
              " 23560,\n",
              " 23561,\n",
              " 23562,\n",
              " 23563,\n",
              " 23564,\n",
              " 23565,\n",
              " 23566,\n",
              " 23567,\n",
              " 23568,\n",
              " 23569,\n",
              " 23570,\n",
              " 23571,\n",
              " 23572,\n",
              " 23573,\n",
              " 23574,\n",
              " 23575,\n",
              " 23576,\n",
              " 23577,\n",
              " 23578,\n",
              " 23579,\n",
              " 23580,\n",
              " 23581,\n",
              " 23582,\n",
              " 23583,\n",
              " 23584,\n",
              " 23585,\n",
              " 23586,\n",
              " 23587,\n",
              " 23588,\n",
              " 23589,\n",
              " 23590,\n",
              " 23591,\n",
              " 23592,\n",
              " 23593,\n",
              " 23594,\n",
              " 23595,\n",
              " 23596,\n",
              " 23597,\n",
              " 23598,\n",
              " 23599,\n",
              " 23600,\n",
              " 23601,\n",
              " 23602,\n",
              " 23603,\n",
              " 23604,\n",
              " 23605,\n",
              " 23606,\n",
              " 23607,\n",
              " 23608,\n",
              " 23609,\n",
              " 23610,\n",
              " 23611,\n",
              " 23612,\n",
              " 23613,\n",
              " 23614,\n",
              " 23615,\n",
              " 23616,\n",
              " 23617,\n",
              " 23618,\n",
              " 23619,\n",
              " 23620,\n",
              " 23621,\n",
              " 23622,\n",
              " 23623,\n",
              " 23624,\n",
              " 23625,\n",
              " 23626,\n",
              " 23627,\n",
              " 23628,\n",
              " 23629,\n",
              " 23630,\n",
              " 23631,\n",
              " 23632,\n",
              " 23633,\n",
              " 23634,\n",
              " 23635,\n",
              " 23636,\n",
              " 23637,\n",
              " 23638,\n",
              " 23639,\n",
              " 23640,\n",
              " 23641,\n",
              " 23642,\n",
              " 23643,\n",
              " 23644,\n",
              " 23645,\n",
              " 23646,\n",
              " 23647,\n",
              " 23648,\n",
              " 23649,\n",
              " 23650,\n",
              " 23651,\n",
              " 23652,\n",
              " 23653,\n",
              " 23654,\n",
              " 23655,\n",
              " 23656,\n",
              " 23657,\n",
              " 23658,\n",
              " 23659,\n",
              " 23660,\n",
              " 23661,\n",
              " 23662,\n",
              " 23663,\n",
              " 23664,\n",
              " 23665,\n",
              " 23666,\n",
              " 23667,\n",
              " 23668,\n",
              " 23669,\n",
              " 23670,\n",
              " 23671,\n",
              " 23672,\n",
              " 23673,\n",
              " 23674,\n",
              " 23675,\n",
              " 23676,\n",
              " 23677,\n",
              " 23678,\n",
              " 23679,\n",
              " 23680,\n",
              " 23681,\n",
              " 23682,\n",
              " 23683,\n",
              " 23684,\n",
              " 23685,\n",
              " 23686,\n",
              " 23687,\n",
              " 23688,\n",
              " 23689,\n",
              " 23690,\n",
              " 23691,\n",
              " 23692,\n",
              " 23693,\n",
              " 23694,\n",
              " 23695,\n",
              " 23696,\n",
              " 23697,\n",
              " 23698,\n",
              " 23699,\n",
              " 23700,\n",
              " 23701,\n",
              " 23702,\n",
              " 23703,\n",
              " 23704,\n",
              " 23705,\n",
              " 23706,\n",
              " 23707,\n",
              " 23708,\n",
              " 23709,\n",
              " 23710,\n",
              " 23711,\n",
              " 23712,\n",
              " 23713,\n",
              " 23714,\n",
              " 23715,\n",
              " 23716,\n",
              " 23717,\n",
              " 23718,\n",
              " 23719,\n",
              " 23720,\n",
              " 23721,\n",
              " 23722,\n",
              " 23723,\n",
              " 23724,\n",
              " 23725,\n",
              " 23726,\n",
              " 23727,\n",
              " 23728,\n",
              " 23729,\n",
              " 23730,\n",
              " 23731,\n",
              " 23732,\n",
              " 23733,\n",
              " 23734,\n",
              " 23735,\n",
              " 23736,\n",
              " 23737,\n",
              " 23738,\n",
              " 23739,\n",
              " 23740,\n",
              " 23741,\n",
              " 23742,\n",
              " 23743,\n",
              " 23744,\n",
              " 23745,\n",
              " 23746,\n",
              " 23747,\n",
              " 23748,\n",
              " 23749,\n",
              " 23750,\n",
              " 23751,\n",
              " 23752,\n",
              " 23753,\n",
              " 23754,\n",
              " 23755,\n",
              " 23756,\n",
              " 23757,\n",
              " 23758,\n",
              " 23759,\n",
              " 23760,\n",
              " 23761,\n",
              " 23762,\n",
              " 23763,\n",
              " 23764,\n",
              " 23765,\n",
              " 23766,\n",
              " 23767,\n",
              " 23768,\n",
              " 23769,\n",
              " 23770,\n",
              " 23771,\n",
              " 23772,\n",
              " 23773,\n",
              " 23774,\n",
              " 23775,\n",
              " 23776,\n",
              " 23777,\n",
              " 23778,\n",
              " 23779,\n",
              " 23780,\n",
              " 23781,\n",
              " 23782,\n",
              " 23783,\n",
              " 23784,\n",
              " 23785,\n",
              " 23786,\n",
              " 23787,\n",
              " 23788,\n",
              " 23789,\n",
              " 23790,\n",
              " 23791,\n",
              " 23792,\n",
              " 23793,\n",
              " 23794,\n",
              " 23795,\n",
              " 23796,\n",
              " 23797,\n",
              " 23798,\n",
              " 23799,\n",
              " 23800,\n",
              " 23801,\n",
              " 23802,\n",
              " 23803,\n",
              " 23804,\n",
              " 23805,\n",
              " 23806,\n",
              " 23807,\n",
              " 23808,\n",
              " 23809,\n",
              " 23810,\n",
              " 23811,\n",
              " 23812,\n",
              " 23813,\n",
              " 23814,\n",
              " 23815,\n",
              " 23816,\n",
              " 23817,\n",
              " 23818,\n",
              " 23819,\n",
              " 23820,\n",
              " 23821,\n",
              " 23822,\n",
              " 23823,\n",
              " 23824,\n",
              " 23825,\n",
              " 23826,\n",
              " 23827,\n",
              " 23828,\n",
              " 23829,\n",
              " 23830,\n",
              " 23831,\n",
              " 23832,\n",
              " 23833,\n",
              " 23834,\n",
              " 23835,\n",
              " 23836,\n",
              " 23837,\n",
              " 23838,\n",
              " 23839,\n",
              " 23840,\n",
              " 23841,\n",
              " 23842,\n",
              " 23843,\n",
              " 23844,\n",
              " 23845,\n",
              " 23846,\n",
              " 23847,\n",
              " 23848,\n",
              " 23849,\n",
              " 23850,\n",
              " 23851,\n",
              " 23852,\n",
              " 23853,\n",
              " 23854,\n",
              " 23855,\n",
              " 23856,\n",
              " 23857,\n",
              " 23858,\n",
              " 23859,\n",
              " 23860,\n",
              " 23861,\n",
              " 23862,\n",
              " 23863,\n",
              " 23864,\n",
              " 23865,\n",
              " 23866,\n",
              " 23867,\n",
              " 23868,\n",
              " 23869,\n",
              " 23870,\n",
              " 23871,\n",
              " 23872,\n",
              " 23873,\n",
              " 23874,\n",
              " 23875,\n",
              " 23876,\n",
              " 23877,\n",
              " 23878,\n",
              " 23879,\n",
              " 23880,\n",
              " 23881,\n",
              " 23882,\n",
              " 23883,\n",
              " 23884,\n",
              " 23885,\n",
              " 23886,\n",
              " 23887,\n",
              " 23888,\n",
              " 23889,\n",
              " 23890,\n",
              " 23891,\n",
              " 23892,\n",
              " 23893,\n",
              " 23894,\n",
              " 23895,\n",
              " 23896,\n",
              " 23897,\n",
              " 23898,\n",
              " 23899,\n",
              " 23900,\n",
              " 23901,\n",
              " 23902,\n",
              " 23903,\n",
              " 23904,\n",
              " 23905,\n",
              " 23906,\n",
              " 23907,\n",
              " 23908,\n",
              " 23909,\n",
              " 23910,\n",
              " 23911,\n",
              " 23912,\n",
              " 23913,\n",
              " 23914,\n",
              " 23915,\n",
              " 23916,\n",
              " 23917,\n",
              " 23918,\n",
              " 23919,\n",
              " 23920,\n",
              " 23921,\n",
              " 23922,\n",
              " 23923,\n",
              " 23924,\n",
              " 23925,\n",
              " 23926,\n",
              " 23927,\n",
              " 23928,\n",
              " 23929,\n",
              " 23930,\n",
              " 23931,\n",
              " 23932,\n",
              " 23933,\n",
              " 23934,\n",
              " 23935,\n",
              " 23936,\n",
              " 23937,\n",
              " 23938,\n",
              " 23939,\n",
              " 23940,\n",
              " 23941,\n",
              " 23942,\n",
              " 23943,\n",
              " 23944,\n",
              " 23945,\n",
              " 23946,\n",
              " 23947,\n",
              " 23948,\n",
              " 23949,\n",
              " 23950,\n",
              " 23951,\n",
              " 23952,\n",
              " 23953,\n",
              " 23954,\n",
              " 23955,\n",
              " 23956,\n",
              " 23957,\n",
              " 23958,\n",
              " 23959,\n",
              " 23960,\n",
              " 23961,\n",
              " 23962,\n",
              " 23963,\n",
              " 23964,\n",
              " 23965,\n",
              " 23966,\n",
              " 23967,\n",
              " 23968,\n",
              " 23969,\n",
              " 23970,\n",
              " 23971,\n",
              " 23972,\n",
              " 23973,\n",
              " 23974,\n",
              " 23975,\n",
              " 23976,\n",
              " 23977,\n",
              " 23978,\n",
              " 23979,\n",
              " 23980,\n",
              " 23981,\n",
              " 23982,\n",
              " 23983,\n",
              " 23984,\n",
              " 23985,\n",
              " 23986,\n",
              " 23987,\n",
              " 23988,\n",
              " 23989,\n",
              " 23990,\n",
              " 23991,\n",
              " 23992,\n",
              " 23993,\n",
              " 23994,\n",
              " 23995,\n",
              " 23996,\n",
              " 23997,\n",
              " 23998,\n",
              " 23999,\n",
              " 24000,\n",
              " 24001,\n",
              " 24002,\n",
              " 24003,\n",
              " 24004,\n",
              " 24005,\n",
              " 24006,\n",
              " 24007,\n",
              " 24008,\n",
              " 24009,\n",
              " 24010,\n",
              " 24011,\n",
              " 24012,\n",
              " 24013,\n",
              " 24014,\n",
              " 24015,\n",
              " 24016,\n",
              " 24017,\n",
              " 24018,\n",
              " 24019,\n",
              " 24020,\n",
              " 24021,\n",
              " 24022,\n",
              " 24023,\n",
              " 24024,\n",
              " 24025,\n",
              " 24026,\n",
              " 24027,\n",
              " 24028,\n",
              " 24029,\n",
              " 24030,\n",
              " 24031,\n",
              " 24032,\n",
              " 24033,\n",
              " 24034,\n",
              " 24035,\n",
              " 24036,\n",
              " 24037,\n",
              " 24038,\n",
              " 24039,\n",
              " 24040,\n",
              " 24041,\n",
              " 24042,\n",
              " 24043,\n",
              " 24044,\n",
              " 24045,\n",
              " 24046,\n",
              " 24047,\n",
              " 24048,\n",
              " 24049,\n",
              " 24050,\n",
              " 24051,\n",
              " 24052,\n",
              " 24053,\n",
              " 24054,\n",
              " 24055,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V6a1D_fCBg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "15576253-394d-4ca0-8eff-0550ca34219f"
      },
      "source": [
        "\n",
        "output_df = evaluate_dataset(all_data.loc[val_indices])\n",
        "output_df"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   poetry_ground_truth  \\\n",
              "0              شد از خویش و پیوند و سرشت / کردهای در چشم او هر خوب خبر   \n",
              "1            بردهای از خویش و پاک و سرشت / کردهای در چشم او هر خوب روی   \n",
              "2               هر چه محسوس است او رد میکند / و یا ناپیداست مسند میکند   \n",
              "3               هر چه محسوس است او رد میکند / و تو ناپیداست مسند میکند   \n",
              "4                  عشق او پیدا و معشوقش نهان / هم بیرون شده او در جهان   \n",
              "...                                                                ...   \n",
              "5759             ابر آب داد و درختان چمن را / شاخ برهنه پیرهن گیاه کرد   \n",
              "5760            ابر ار داد بیخ درختان مرده را / شاخ برهنه شاخ تازه کرد   \n",
              "5761             توحیدگوی او نه گفت و بس / هر گل که غنچه بر شاخسار کرد   \n",
              "5762      دیدار یار چه دانی چه ذوق دارد / هر که در بیابان بر خود ببارد   \n",
              "5763  دیدار یار غایب را چه امکان دارد / ابری که در دلم بر تشنهای ببارد   \n",
              "\n",
              "                                                                                                                                                                                    text  \\\n",
              "0                انان را از اعمال و عادات و طبیعتشان بردهای یعنی خلاصشان کردهای. و چیز خوب و زیبا را در نظر انان زشت و بد جلوه دادهاست. به عبارت دیگر از حیز محسوسات خارج و فرار کردهای.   \n",
              "1                خود را از خویشتن و خویشاوندان و اقوام بردهای یعنی خلاصشان کردهای. و چیز خوب و زیبا را در نظر خویش زشت و بد جلوه دادهای. به عبارت دیگر از دیار خود گریخته و فرار کردهای.   \n",
              "2                                                                                                  چنین عارف عارفی هر چیز ضروری و قریب به عالم باطنی را پس زده و به عالم غیب تکیه میکند.   \n",
              "3                                                                                                      چنین و عارفی هر چیز محسوس و متعلق به ان است را پس میزند و به عالم غیب رجوع میکند.   \n",
              "4                                                چنین کسی عشق و دوستی اش اشکار و معشوقش پنهان است. به همین مناسبت حضرت علی در بیرون از این بند پنهان است ولی میل به عشق او در همین است..   \n",
              "...                                                                                                                                                                                  ...   \n",
              "5759                                      سپس تا مدتها درختان مرده را خشک کرده و بار دیگر درختان مرده زمستانی را ابیاری کرده و لباس فصل بهار را که پر از گل و گیاه است بر تن خود میکنند.   \n",
              "5760                                   ابرها تا حدی درختان مرده را قطع میکند و بار دیگر درختان خشک زمستانی را خشک کرده و لباسهای فصل بهار را که پر از گل و گیاه است بر تن انان میپوشاند.   \n",
              "5761  تنها انسانها نیستند که خداوند را یکتا میدانند و ذکر او را میگویند ؛ بلکه تمام موجودات عالم به تسبیح او میپردازند. هر بلبلی که روی درختان نماز میخواند درواقع به ذکر خدا مشغول است.   \n",
              "5762                                                                                           اما میدانید که کار کسی که تاکنون غایب بوده چقدر سرگرم کننده است و همه را بر سر ذوق میاورد   \n",
              "5763                                                                                           تو میدانی که دیدن کسی که مدتها جوان بوده واقعا خوشحال کننده است و همه را بر سر ذوق میاورد   \n",
              "\n",
              "                                                            poetry_generated_Seq2Seq_with_Att  \n",
              "0                                                                    تا که که در تو را <end>   \n",
              "1                                                                    تا که که در تو را <end>   \n",
              "2     تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا   \n",
              "3     تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا   \n",
              "4     تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا   \n",
              "...                                                                                       ...  \n",
              "5759  تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا   \n",
              "5760                                                                 تا که که در تو را <end>   \n",
              "5761  تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا که که در تو <sep> تا   \n",
              "5762                                                                 تا که که در تو را <end>   \n",
              "5763                                                                 تا که که در تو را <end>   \n",
              "\n",
              "[5764 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83df52e0-dd2c-42a0-a3f6-06491999e7ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>poetry_ground_truth</th>\n",
              "      <th>text</th>\n",
              "      <th>poetry_generated_Seq2Seq_with_Att</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>شد از خویش و پیوند و سرشت / کردهای در چشم او هر خوب خبر</td>\n",
              "      <td>انان را از اعمال و عادات و طبیعتشان بردهای یعنی خلاصشان کردهای. و چیز خوب و زیبا را در نظر انان زشت و بد جلوه دادهاست. به عبارت دیگر از حیز محسوسات خارج و فرار کردهای.</td>\n",
              "      <td>تا که که در تو را &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>بردهای از خویش و پاک و سرشت / کردهای در چشم او هر خوب روی</td>\n",
              "      <td>خود را از خویشتن و خویشاوندان و اقوام بردهای یعنی خلاصشان کردهای. و چیز خوب و زیبا را در نظر خویش زشت و بد جلوه دادهای. به عبارت دیگر از دیار خود گریخته و فرار کردهای.</td>\n",
              "      <td>تا که که در تو را &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>هر چه محسوس است او رد میکند / و یا ناپیداست مسند میکند</td>\n",
              "      <td>چنین عارف عارفی هر چیز ضروری و قریب به عالم باطنی را پس زده و به عالم غیب تکیه میکند.</td>\n",
              "      <td>تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>هر چه محسوس است او رد میکند / و تو ناپیداست مسند میکند</td>\n",
              "      <td>چنین و عارفی هر چیز محسوس و متعلق به ان است را پس میزند و به عالم غیب رجوع میکند.</td>\n",
              "      <td>تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>عشق او پیدا و معشوقش نهان / هم بیرون شده او در جهان</td>\n",
              "      <td>چنین کسی عشق و دوستی اش اشکار و معشوقش پنهان است. به همین مناسبت حضرت علی در بیرون از این بند پنهان است ولی میل به عشق او در همین است..</td>\n",
              "      <td>تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5759</th>\n",
              "      <td>ابر آب داد و درختان چمن را / شاخ برهنه پیرهن گیاه کرد</td>\n",
              "      <td>سپس تا مدتها درختان مرده را خشک کرده و بار دیگر درختان مرده زمستانی را ابیاری کرده و لباس فصل بهار را که پر از گل و گیاه است بر تن خود میکنند.</td>\n",
              "      <td>تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5760</th>\n",
              "      <td>ابر ار داد بیخ درختان مرده را / شاخ برهنه شاخ تازه کرد</td>\n",
              "      <td>ابرها تا حدی درختان مرده را قطع میکند و بار دیگر درختان خشک زمستانی را خشک کرده و لباسهای فصل بهار را که پر از گل و گیاه است بر تن انان میپوشاند.</td>\n",
              "      <td>تا که که در تو را &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5761</th>\n",
              "      <td>توحیدگوی او نه گفت و بس / هر گل که غنچه بر شاخسار کرد</td>\n",
              "      <td>تنها انسانها نیستند که خداوند را یکتا میدانند و ذکر او را میگویند ؛ بلکه تمام موجودات عالم به تسبیح او میپردازند. هر بلبلی که روی درختان نماز میخواند درواقع به ذکر خدا مشغول است.</td>\n",
              "      <td>تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا که که در تو &lt;sep&gt; تا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5762</th>\n",
              "      <td>دیدار یار چه دانی چه ذوق دارد / هر که در بیابان بر خود ببارد</td>\n",
              "      <td>اما میدانید که کار کسی که تاکنون غایب بوده چقدر سرگرم کننده است و همه را بر سر ذوق میاورد</td>\n",
              "      <td>تا که که در تو را &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5763</th>\n",
              "      <td>دیدار یار غایب را چه امکان دارد / ابری که در دلم بر تشنهای ببارد</td>\n",
              "      <td>تو میدانی که دیدن کسی که مدتها جوان بوده واقعا خوشحال کننده است و همه را بر سر ذوق میاورد</td>\n",
              "      <td>تا که که در تو را &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5764 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83df52e0-dd2c-42a0-a3f6-06491999e7ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83df52e0-dd2c-42a0-a3f6-06491999e7ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83df52e0-dd2c-42a0-a3f6-06491999e7ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-887002f4-0617-4270-9880-e911c96878bf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-887002f4-0617-4270-9880-e911c96878bf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-887002f4-0617-4270-9880-e911c96878bf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQBNvR3iB4fV"
      },
      "source": [
        "output_df.to_csv(f'./Data/Seq2Seq_with_att_{epochs}_epochs_{batch_s}_batch_s_{embedding_dim}_embedding_dim_{units}_units_.csv',\n",
        "                 index=False)"
      ],
      "execution_count": 50,
      "outputs": []
    }
  ]
}